{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import importlib.util\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import typing\n",
    "\n",
    "import airbyte as ab\n",
    "import pathway as pw\n",
    "from pathway.xpacks.llm import embedders, parsers, splitters, vector_store\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "# from unstructured.documents.elements import Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure libmagic is available\n",
    "LIBMAGIC_AVAILABLE = bool(importlib.util.find_spec(\"magic\"))\n",
    "assert LIBMAGIC_AVAILABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv.dotenv_values('.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_name = 'aave'\n",
    "prop_order_by = 'asc'\n",
    "api_key = config['BOARDROOM_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbyteSchema(pw.Schema):\n",
    "    _airbyte_raw_id: str\n",
    "    _airbyte_extracted_at: pw.DateTimeNaive\n",
    "    _airbyte_meta: dict\n",
    "\n",
    "class BoardroomAPI(AirbyteSchema):\n",
    "    stream: str\n",
    "    data: pw.Json\n",
    "    nextcursor: str | None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airbyte.caches import CacheBase\n",
    "from airbyte.sources import Source, get_source\n",
    "import pandas as pd\n",
    "from pathway.io.python import ConnectorSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbyteAPIConector(ConnectorSubject):\n",
    "\n",
    "    source: Source\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Airbyte specific\n",
    "        name: str,\n",
    "        config: dict[str, typing.Any] | None = None,\n",
    "        *args,\n",
    "        streams: str | list[str] | None = None,\n",
    "        version: str | None = None,\n",
    "        source_manifest: bool | dict | pathlib.Path | str = False,\n",
    "        install_if_missing: bool = True,\n",
    "        install_root: pathlib.Path | None = None,\n",
    "        # Airbyte source.read specific\n",
    "        cache: CacheBase | None = None,\n",
    "        force_full_refresh: bool = False,\n",
    "        skip_validation: bool = False,\n",
    "        # Pathway specific\n",
    "        # mode: str, # TODO: how to perform streaming vs static?\n",
    "        # refresh_interval_ms: int, # used for time.sleep. This is handled by airbyte through correct params in manifest\n",
    "        # *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.source = get_source(\n",
    "            name=name,\n",
    "            config=config,\n",
    "            streams=streams,\n",
    "            source_manifest=source_manifest,\n",
    "            install_if_missing=install_if_missing,\n",
    "            install_root=install_root\n",
    "        )\n",
    "        # check source yaml definition is correct\n",
    "        self.source.check()\n",
    "\n",
    "        # select streams\n",
    "        if streams: self.source.select_streams(streams)\n",
    "        else: self.source.select_all_streams()\n",
    "\n",
    "        # read parameters\n",
    "        self.cache = cache\n",
    "        self.force_full_refresh = force_full_refresh\n",
    "        self.skip_validation = skip_validation\n",
    "\n",
    "        # connector parameters\n",
    "        # self.mode = mode\n",
    "        # self.refresh_interval = refresh_interval_ms / 1000.0\n",
    "\n",
    "    def run(self):\n",
    "        result = self.source.read(\n",
    "            cache=self.cache,\n",
    "            force_full_refresh=self.force_full_refresh,\n",
    "            skip_validation=self.skip_validation\n",
    "        )\n",
    "        for stream_name, data in result.streams.items():\n",
    "            # NOTICE: workaround to remove duplicate records that contains the latest page\n",
    "            # with null cursor. This happens when running multiple times with force_full_refresh\n",
    "            # TODO: how to clean up cache completely so recods with null nextCursor are removed\n",
    "            df = data.to_pandas()\n",
    "            df_clean = df.dropna(how='any', ignore_index=True) # drop record pages with null nextCursor\n",
    "            df_null = df[df.isna().any(axis=1)] # get record pages with null nextCursor\n",
    "            df_final = pd.concat([\n",
    "                df_clean,\n",
    "                df_null.iloc[[0], :] # include the latest page\n",
    "            ])\n",
    "            records = df_final.to_dict(orient='records')\n",
    "\n",
    "            for row in records:\n",
    "                self.next(stream=stream_name, **row)\n",
    "            # for row in list(data):\n",
    "            #     self.next(stream=stream_name, **dict(row))\n",
    "            \n",
    "\n",
    "    def on_stop(self):\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Parser UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from io import BytesIO\n",
    "from pathway.optional_import import optional_imports\n",
    "# from typing import TYPE_CHECKING, Any, Literal\n",
    "\n",
    "from typing import Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomParseUnstructured(pw.UDF):\n",
    "    \"\"\"\n",
    "    Parse document using `https://unstructured.io/ <https://unstructured.io/>`_.\n",
    "\n",
    "    All arguments can be overridden during UDF application.\n",
    "\n",
    "    Args:\n",
    "        - mode: single, elements or paged.\n",
    "          When single, each document is parsed as one long text string.\n",
    "          When elements, each document is split into unstructured's elements.\n",
    "          When paged, each pages's text is separately extracted.\n",
    "        - post_processors: list of callables that will be applied to all extracted texts.\n",
    "        - **unstructured_kwargs: extra kwargs to be passed to unstructured.io's `partition` function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str = \"single\",\n",
    "        post_processors: list[Callable] | None = None,\n",
    "        **unstructured_kwargs: Any,\n",
    "    ):\n",
    "        with optional_imports(\"xpack-llm-docs\"):\n",
    "            import unstructured.partition.auto  # noqa:F401\n",
    "\n",
    "        super().__init__()\n",
    "        _valid_modes = {\"single\", \"elements\", \"paged\"}\n",
    "        if mode not in _valid_modes:\n",
    "            raise ValueError(\n",
    "                f\"Got {mode} for `mode`, but should be one of `{_valid_modes}`\"\n",
    "            )\n",
    "\n",
    "        self.kwargs = dict(\n",
    "            mode=mode,\n",
    "            post_processors=post_processors or [],\n",
    "            unstructured_kwargs=unstructured_kwargs,\n",
    "        )\n",
    "\n",
    "    # # `links` and `languages` in metadata are lists, so their content should be added.\n",
    "    # # We don't want return `coordinates`, `parent_id` and `category_depth` - these are\n",
    "    # # element specific (i.e. they can differ for elements on the same page)\n",
    "    # def _combine_metadata(self, left: dict, right: dict) -> dict:\n",
    "    #     result = {}\n",
    "    #     links = left.pop(\"links\", []) + right.pop(\"links\", [])\n",
    "    #     languages = list(set(left.pop(\"languages\", []) + right.pop(\"languages\", [])))\n",
    "    #     result.update(left)\n",
    "    #     result.update(right)\n",
    "    #     result[\"links\"] = links\n",
    "    #     result[\"languages\"] = languages\n",
    "    #     result.pop(\"coordinates\", None)\n",
    "    #     result.pop(\"parent_id\", None)\n",
    "    #     result.pop(\"category_depth\", None)\n",
    "    #     return result\n",
    "\n",
    "    # def __wrapped__(self, contents: bytes, **kwargs) -> list[tuple[str, dict]]:\n",
    "    def __wrapped__(self, contents: bytes, **kwargs) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Parse the given document:\n",
    "\n",
    "        Args:\n",
    "            - contents: document contents\n",
    "            - **kwargs: override for defaults set in the constructor\n",
    "\n",
    "        Returns:\n",
    "            a list of pairs: text chunk and metadata\n",
    "            The metadata is obtained from Unstructured, you can check possible values\n",
    "            in the `Unstructed documentation <https://unstructured-io.github.io/unstructured/metadata.html>`\n",
    "            Note that when `mode` is set to `single` or `paged` some of these fields are\n",
    "            removed if they are specific to a single element, e.g. `category_depth`.\n",
    "        \"\"\"\n",
    "        import unstructured.partition.auto\n",
    "\n",
    "        kwargs = {**self.kwargs, **kwargs}\n",
    "\n",
    "        # print(\"kwargs\", kwargs)\n",
    "\n",
    "        elements = unstructured.partition.auto.partition(\n",
    "            file=BytesIO(contents), **kwargs.pop(\"unstructured_kwargs\")\n",
    "        )\n",
    "\n",
    "        post_processors = kwargs.pop(\"post_processors\")\n",
    "        for element in elements:\n",
    "            for post_processor in post_processors:\n",
    "                element.apply(post_processor)\n",
    "\n",
    "        mode = kwargs.pop(\"mode\")\n",
    "\n",
    "        if kwargs:\n",
    "            raise ValueError(f\"Unknown arguments: {', '.join(kwargs.keys())}\")\n",
    "\n",
    "        if mode == \"elements\":\n",
    "            # docs: list[tuple[str, dict]] = list()\n",
    "            # for element in elements:\n",
    "            #     # NOTE(MthwRobinson) - the attribute check is for backward compatibility\n",
    "            #     # with unstructured<0.4.9. The metadata attributed was added in 0.4.9.\n",
    "            #     # if hasattr(element, \"metadata\"):\n",
    "            #     #     metadata = element.metadata.to_dict()\n",
    "            #     # else:\n",
    "            #     #     metadata = {}\n",
    "            #     # if hasattr(element, \"category\"):\n",
    "            #     #     metadata[\"category\"] = element.category\n",
    "            #     # docs.append((str(element), metadata))\n",
    "            docs: list[dict] = [el.to_dict() for el in elements]\n",
    "        # elif mode == \"paged\":\n",
    "        #     text_dict: dict[int, str] = {}\n",
    "        #     meta_dict: dict[int, dict] = {}\n",
    "\n",
    "        #     for idx, element in enumerate(elements):\n",
    "        #         if hasattr(element, \"metadata\"):\n",
    "        #             metadata = element.metadata.to_dict()\n",
    "        #         else:\n",
    "        #             metadata = {}\n",
    "        #         page_number = metadata.get(\"page_number\", 1)\n",
    "\n",
    "        #         # Check if this page_number already exists in docs_dict\n",
    "        #         if page_number not in text_dict:\n",
    "        #             # If not, create new entry with initial text and metadata\n",
    "        #             text_dict[page_number] = str(element) + \"\\n\\n\"\n",
    "        #             meta_dict[page_number] = metadata\n",
    "        #         else:\n",
    "        #             # If exists, append to text and update the metadata\n",
    "        #             text_dict[page_number] += str(element) + \"\\n\\n\"\n",
    "        #             meta_dict[page_number] = self._combine_metadata(\n",
    "        #                 meta_dict[page_number], metadata\n",
    "        #             )\n",
    "\n",
    "        #     # Convert the dict to a list of dicts representing documents\n",
    "        #     docs = [(text_dict[key], meta_dict[key]) for key in text_dict.keys()]\n",
    "        # elif mode == \"single\":\n",
    "        #     metadata = {}\n",
    "        #     for element in elements:\n",
    "        #         if hasattr(element, \"metadata\"):\n",
    "        #             metadata = self._combine_metadata(\n",
    "        #                 metadata, element.metadata.to_dict()\n",
    "        #             )\n",
    "        #     text = \"\\n\\n\".join([str(el) for el in elements])\n",
    "        #     docs = [(text, metadata)]\n",
    "        else:\n",
    "            raise ValueError(f\"mode of {mode} not supported.\")\n",
    "        return docs\n",
    "\n",
    "    def __call__(self, contents: pw.ColumnExpression, **kwargs) -> pw.ColumnExpression:\n",
    "        \"\"\"\n",
    "        Parse the given document.\n",
    "\n",
    "        Args:\n",
    "            - contents: document contents\n",
    "            - **kwargs: override for defaults set in the constructor\n",
    "\n",
    "        Returns:\n",
    "            A column with a list of pairs for each query. Each pair is a text chunk and\n",
    "            associated metadata.\n",
    "            The metadata is obtained from Unstructured, you can check possible values\n",
    "            in the `Unstructed documentation <https://unstructured-io.github.io/unstructured/metadata.html>`\n",
    "            Note that when `mode` is set to `single` or `paged` some of these fields are\n",
    "            removed if they are specific to a single element, e.g. `category_depth`.\n",
    "        \"\"\"\n",
    "        return super().__call__(contents, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export CFLAGS=\"-Wno-nullability-completeness\" if trying to install pillow-heif missingn module\n",
    "# libmagic -> Required for having libmagic working:\n",
    "# - brew install libmagic\n",
    "# - pip install python-magic-bin\n",
    "\n",
    "# parser = parsers.ParseUnstructured(mode=\"elements\")\n",
    "parser = CustomParseUnstructured(mode=\"elements\") # TODO: do we need extra cleaning function as post_processors ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Embedder UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import contextvars\n",
    "from functools import partial\n",
    "from litellm import (\n",
    "    client,\n",
    "    embedding,\n",
    "    exception_type,\n",
    ")\n",
    "from litellm.utils import EmbeddingResponse\n",
    "\n",
    "from pathway.xpacks.llm.embedders import BaseEmbedder, _monkeypatch_openai_async\n",
    "from pathway.internals import udfs\n",
    "from pathway.optional_import import optional_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@client\n",
    "async def aembedding(*args, **kwargs) -> EmbeddingResponse:\n",
    "    \"\"\"\n",
    "    Asynchronously calls the `embedding` function with the given arguments and keyword arguments.\n",
    "\n",
    "    Parameters:\n",
    "    - `args` (tuple): Positional arguments to be passed to the `embedding` function.\n",
    "    - `kwargs` (dict): Keyword arguments to be passed to the `embedding` function.\n",
    "\n",
    "    Returns:\n",
    "    - `response` (Any): The response returned by the `embedding` function.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    model = args[0] if len(args) > 0 else kwargs[\"model\"]\n",
    "    ### PASS ARGS TO Embedding ###\n",
    "    kwargs[\"aembedding\"] = True\n",
    "    \n",
    "    # custom_llm_provider = None\n",
    "    custom_llm_provider = kwargs.get(\"custom_llm_provider\", None) # NOTICE: required update\n",
    "\n",
    "    try:\n",
    "        # Use a partial function to pass your keyword arguments\n",
    "        func = partial(embedding, *args, **kwargs)\n",
    "\n",
    "        # Add the context to the function\n",
    "        ctx = contextvars.copy_context()\n",
    "        func_with_context = partial(ctx.run, func)\n",
    "\n",
    "        # NOTICE: don't need that\n",
    "        # _, custom_llm_provider, _, _ = get_llm_provider(\n",
    "        #     model=model, api_base=kwargs.get(\"api_base\", None)\n",
    "        # )\n",
    "\n",
    "        if (\n",
    "            custom_llm_provider == \"openai\"\n",
    "            # or custom_llm_provider == \"azure\"\n",
    "            # or custom_llm_provider == \"xinference\"\n",
    "            # or custom_llm_provider == \"voyage\"\n",
    "            # or custom_llm_provider == \"mistral\"\n",
    "            # or custom_llm_provider == \"custom_openai\"\n",
    "            # or custom_llm_provider == \"triton\"\n",
    "            # or custom_llm_provider == \"anyscale\"\n",
    "            # or custom_llm_provider == \"openrouter\"\n",
    "            # or custom_llm_provider == \"deepinfra\"\n",
    "            # or custom_llm_provider == \"perplexity\"\n",
    "            # or custom_llm_provider == \"groq\"\n",
    "            # or custom_llm_provider == \"nvidia_nim\"\n",
    "            # or custom_llm_provider == \"volcengine\"\n",
    "            # or custom_llm_provider == \"deepseek\"\n",
    "            # or custom_llm_provider == \"fireworks_ai\"\n",
    "            # or custom_llm_provider == \"ollama\"\n",
    "            # or custom_llm_provider == \"vertex_ai\"\n",
    "            # or custom_llm_provider == \"databricks\"\n",
    "            # or custom_llm_provider == \"watsonx\"\n",
    "        ):  # currently implemented aiohttp calls for just azure and openai, soon all.\n",
    "            # Await normally\n",
    "            init_response = await loop.run_in_executor(None, func_with_context)\n",
    "            if isinstance(init_response, dict):\n",
    "                response = EmbeddingResponse(**init_response)\n",
    "            elif isinstance(init_response, EmbeddingResponse):  ## CACHING SCENARIO\n",
    "                response = init_response\n",
    "            elif asyncio.iscoroutine(init_response):\n",
    "                response = await init_response\n",
    "        else:\n",
    "            # Call the synchronous function using run_in_executor\n",
    "            response = await loop.run_in_executor(None, func_with_context)\n",
    "        if response is not None and hasattr(response, \"_hidden_params\"):\n",
    "            response._hidden_params[\"custom_llm_provider\"] = custom_llm_provider\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        custom_llm_provider = custom_llm_provider or \"openai\"\n",
    "        raise exception_type(\n",
    "            model=model,\n",
    "            custom_llm_provider=custom_llm_provider,\n",
    "            original_exception=e,\n",
    "            completion_kwargs=args,\n",
    "            extra_kwargs=kwargs,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLiteLLMEmbedder(BaseEmbedder):\n",
    "    \"\"\"Pathway wrapper for `litellm.embedding`.\n",
    "\n",
    "    Model has to be specified either in constructor call or in each application, no default\n",
    "    is provided. The capacity, retry_strategy and cache_strategy need to be specified\n",
    "    during object construction. All other arguments can be overridden during application.\n",
    "\n",
    "    Args:\n",
    "        - capacity: Maximum number of concurrent operations allowed.\n",
    "            Defaults to None, indicating no specific limit.\n",
    "        - retry_strategy: Strategy for handling retries in case of failures.\n",
    "            Defaults to None, meaning no retries.\n",
    "        - cache_strategy: Defines the caching mechanism. To enable caching,\n",
    "            a valid `CacheStrategy` should be provided.\n",
    "            See `Cache strategy <https://pathway.com/developers/api-docs/udfs#pathway.udfs.CacheStrategy>`_\n",
    "            for more information. Defaults to None.\n",
    "        - model: The embedding model to use.\n",
    "        - timeout: The timeout value for the API call, default 10 mins\n",
    "        - litellm_call_id: The call ID for litellm logging.\n",
    "        - litellm_logging_obj: The litellm logging object.\n",
    "        - logger_fn: The logger function.\n",
    "        - api_base: Optional. The base URL for the API.\n",
    "        - api_version: Optional. The version of the API.\n",
    "        - api_key: Optional. The API key to use.\n",
    "        - api_type: Optional. The type of the API.\n",
    "        - custom_llm_provider: The custom llm provider.\n",
    "\n",
    "    Any arguments can be provided either to the constructor or in the UDF call.\n",
    "    To specify the `model` in the UDF call, set it to None.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> import pathway as pw\n",
    "    >>> from pathway.xpacks.llm import embedders\n",
    "    >>> embedder = embedders.LiteLLMEmbedder(model=\"text-embedding-ada-002\")\n",
    "    >>> t = pw.debug.table_from_markdown('''\n",
    "    ... txt\n",
    "    ... Text\n",
    "    ... ''')\n",
    "    >>> t.select(ret=embedder(pw.this.txt))\n",
    "    <pathway.Table schema={'ret': list[float]}>\n",
    "\n",
    "    >>> import pathway as pw\n",
    "    >>> from pathway.xpacks.llm import embedders\n",
    "    >>> embedder = embedders.LiteLLMEmbedder()\n",
    "    >>> t = pw.debug.table_from_markdown('''\n",
    "    ... txt  | model\n",
    "    ... Text | text-embedding-ada-002\n",
    "    ... ''')\n",
    "    >>> t.select(ret=embedder(pw.this.txt, model=pw.this.model))\n",
    "    <pathway.Table schema={'ret': list[float]}>\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        capacity: int | None = None,\n",
    "        retry_strategy: udfs.AsyncRetryStrategy | None = None,\n",
    "        cache_strategy: udfs.CacheStrategy | None = None,\n",
    "        model: str | None = None,\n",
    "        **llmlite_kwargs,\n",
    "    ):\n",
    "        with optional_imports(\"xpack-llm\"):\n",
    "            import litellm  # noqa:F401\n",
    "\n",
    "        _monkeypatch_openai_async()\n",
    "        executor = udfs.async_executor(capacity=capacity, retry_strategy=retry_strategy)\n",
    "        super().__init__(\n",
    "            executor=executor,\n",
    "            cache_strategy=cache_strategy,\n",
    "        )\n",
    "        self.kwargs = dict(llmlite_kwargs)\n",
    "        if model is not None:\n",
    "            self.kwargs[\"model\"] = model\n",
    "\n",
    "    async def __wrapped__(self, input, **kwargs) -> list[float]:\n",
    "        \"\"\"Embed the documents\n",
    "\n",
    "        Args:\n",
    "            - input: mandatory, the string to embed.\n",
    "            - **kwargs: optional parameters, if unset defaults from the constructor\n",
    "              will be taken.\n",
    "        \"\"\"\n",
    "        # import litellm\n",
    "\n",
    "        kwargs = {**self.kwargs, **kwargs}\n",
    "        # ret = await litellm.aembedding(input=[input or \".\"], **kwargs)\n",
    "        ret = await aembedding(input=[input or \".\"], **kwargs)\n",
    "        return ret.data[0][\"embedding\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad-hoc UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pw.udf\n",
    "def to_json(val: pw.Json) -> pw.Json:\n",
    "    return pw.Json(json.loads(val.as_str()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @pw.udf(executor=pw.udfs.async_executor())\n",
    "\n",
    "@pw.udf\n",
    "def filter_document(document: pw.Json, fields: list[str]) -> pw.Json:\n",
    "    data = { **document.as_dict() }\n",
    "    # data = { \"refId\": document[\"refId\"] }\n",
    "    for field in fields:\n",
    "        if field in data:\n",
    "            data.pop(field)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_logger = logging.getLogger(\"unstructured\")\n",
    "# u_logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONAccumulator(pw.BaseCustomAccumulator):\n",
    "  def __init__(self, initialData: pw.Json):\n",
    "    self.data: list[dict] = list()\n",
    "    self.value: dict = { **initialData.as_dict() }\n",
    "\n",
    "  @classmethod\n",
    "  def from_row(self, row):\n",
    "    [val] = row\n",
    "    return JSONAccumulator(val)\n",
    "\n",
    "  def update(self, other):\n",
    "    self.data.append(other.value)\n",
    "\n",
    "  def compute_result(self) -> list[dict]:\n",
    "    return self.data\n",
    "\n",
    "\n",
    "json_acc = pw.reducers.udf_reducer(JSONAccumulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Protocol(pw.Schema):\n",
    "    cname: str\n",
    "    name: str\n",
    "    categories: str\n",
    "    is_enabled: bool\n",
    "    active_on_website: bool\n",
    "    total_proposals: int\n",
    "    total_votes: int\n",
    "    unique_voters: int\n",
    "    # tokens: list[object]\n",
    "    ptype: str\n",
    "    # delegated_support: dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def protocol_mapper(raw_data: bytes) -> bytes:\n",
    "    # logger.info(raw_data.decode())\n",
    "    data = json.loads(raw_data.decode())[\"data\"]\n",
    "    return json.dumps(\n",
    "        {\n",
    "            \"cname\": data[\"cname\"],\n",
    "            \"name\": data[\"name\"],\n",
    "            \"categories\": \",\".join(data[\"categories\"]),\n",
    "            \"is_enabled\": data[\"isEnabled\"],\n",
    "            \"active_on_website\": data[\"activeOnWebsite\"],\n",
    "            \"total_proposals\": data[\"totalProposals\"],\n",
    "            \"total_votes\": data[\"totalVotes\"],\n",
    "            \"unique_voters\": data[\"uniqueVoters\"],\n",
    "            \"ptype\": data[\"type\"],\n",
    "        }\n",
    "    ).encode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "protocol = pw.io.http.read(\n",
    "    f\"https://api.boardroom.info/v1/protocols/{protocol_name}?key={api_key}\",\n",
    "    method='GET',\n",
    "    headers={\"Accept\": \"application/json\"},\n",
    "    # format=\"raw\",\n",
    "    schema=Protocol,\n",
    "    response_mapper=protocol_mapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pw.udf\n",
    "def append_parent_id(content: pw.Json, parent_id: str) -> pw.Json:\n",
    "    data = { \"parent_id\": parent_id, **content.as_dict() }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pathway HTTP connector (data exploration/test mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICE: need to override base schema\n",
    "# class BoardroomAPI(pw.Schema):\n",
    "#     data: pw.Json\n",
    "#     nextCursor: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposals = pw.io.http.read(\n",
    "#     f\"https://api.boardroom.info/v1/protocols/{protocol_name}/proposals?key={api_key}&orderByIndexedAt{prop_order_by}\",\n",
    "#     method='GET',\n",
    "#     headers={\"Accept\": \"application/json\"},\n",
    "#     format=\"json\",\n",
    "#     schema=BoardroomAPI\n",
    "#     # schema=Proposal,\n",
    "#     # response_mapper=proposal_mapper\n",
    "# )\n",
    "# proposals = proposals.flatten(proposals.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Airbyte Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = ab.caches.new_local_cache(\n",
    "    cache_name='boardroom_cache',\n",
    "    cache_dir='./boardroom_ab_cache',\n",
    "    cleanup=False # NOTICE: CLI param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_connector = AirbyteAPIConector(\n",
    "    name='boardroom-api',\n",
    "    cache=cache,\n",
    "    config={\n",
    "        \"api_key\": api_key, # NOTICE: CLI param\n",
    "        \"cname\": protocol_name, # NOTICE: CLI param\n",
    "        \"page_size\": 1 # TODO: currently not used\n",
    "    },\n",
    "    source_manifest=pathlib.Path(\"../boardroom/connector.yaml\"), # NOTICE: CLI parma\n",
    "    streams=\"proposals\", # NOTICE: doing just an individual strem\n",
    "    force_full_refresh=False, # NOTICE: CLI param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals = pw.io.python.read(\n",
    "    api_connector,\n",
    "    schema=BoardroomAPI\n",
    ")\n",
    "\n",
    "# NOTICE: With Airbyte we need to parse data to Json during pre-processing\n",
    "proposals = proposals.with_columns(\n",
    "    data=to_json(proposals.data)\n",
    ")\n",
    "\n",
    "proposals = proposals.flatten(proposals.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals = proposals.with_columns(\n",
    "    refId=pw.this.data.get(\"refId\", default=pw.Json(\"\")).as_str(),\n",
    "    title=pw.this.data.get(\"title\", default=pw.Json(\"\")).as_str(),\n",
    "    # metadata=pw.apply_with_type(lambda x: filter_document(x, [\"refId\", \"title\", \"content\"]), dict, pw.this.data),\n",
    "    # metadata=pw.apply_with_type(lambda x: filter_document(x), dict, pw.this.data),\n",
    "    metadata=filter_document(pw.this.data, [\"refId\", \"title\", \"content\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_table = proposals.select(\n",
    "    element_id=pw.this.refId,\n",
    "    text=pw.this.title,\n",
    "    metadata=pw.this.metadata,\n",
    "    type=\"Title\"\n",
    "    # content=build_main_element(pw.this.refId, pw.this.title, pw.this.metadata),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_contents = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposal content\n",
    "proposal_contents = proposals.select(\n",
    "    refId=pw.this.refId,\n",
    "    # content=pw.apply_with_type(lambda x: f\"{x}\".encode() if x else b\"\", bytes, pw.this.data.get(\"content\", default=None)),\n",
    "    content=parser(pw.apply_with_type(lambda x: f\"{x.as_str()}\".encode() if x else b\"\", bytes, pw.this.data.get(\"content\", default=None))),\n",
    ")\n",
    "proposal_contents = proposal_contents.flatten(pw.this.content)\n",
    "# # proposals = proposals.select(refId=pw.this.refId, title=pw.this.title, text=pw.this.content[0], metadata=pw.this.content[1])\n",
    "# # proposals = proposals.select(refId=pw.this.refId, title=pw.this.title, text=pw.this.document['text'].as_str(), document=pw.this.document)\n",
    "# proposals = proposals.select(refId=pw.this.refId, document=pw.this.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_contents.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition analysis\n",
    "\n",
    "* Which filetype is detected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.file_utils.filetype import detect_filetype, is_json_processable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheking file type detection during partition\n",
    "\n",
    "@pw.udf\n",
    "def detect(data: pw.Json) -> str:\n",
    "    encoded = data.as_str().encode()\n",
    "    filetype = detect_filetype(file=BytesIO(encoded))\n",
    "    return str(filetype)\n",
    "    \n",
    "\n",
    "meta = proposals.select(\n",
    "    metadata=pw.this.data.get(\"content\", default=None)\n",
    ")\n",
    "meta = meta.with_columns(\n",
    "    filetype=detect(pw.this.metadata),\n",
    ")\n",
    "meta.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# For now, all text data is being recognized as txt files instead of md.\n",
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Grouping partitioned elements by proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_p = proposal_contents.groupby(proposal_contents.refId).reduce(proposal_contents.refId, contents=json_acc(proposal_contents.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_p.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.io.jsonlines.write(grouped_p, \"proposals-partitioned.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition + Chunking content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details on chunking techniques\n",
    "# - basic -> combines sequential elements to maximally fill each chunk\n",
    "# - by_title -> preserves section boundaries and optionally page boundaries\n",
    "# https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "\n",
    "# Chunk Parameters \n",
    "# - Common params\n",
    "include_orig_elements: Optional[bool] = None # Default to True\n",
    "max_characters: Optional[int] = None # hard-max chars per chunk\n",
    "new_after_n_chars: Optional[int] = None # soft-max chars per chunk. Cuts off new sections once they reach a length of n characters (soft max). Defaults to  `max_characters` when not specified, which effectively disables any soft window.\n",
    "overlap: Optional[int] = None # specifies the length of a string (\"tail\") to be drawn from each chunk and prefixed to the next chunk as a context-preserving mechanism. Must be <= `max_characters`\n",
    "overlap_all: Optional[bool] = None # Default to False. Apply overlap between \"normal\" chunks formed from whole elements and not subject to text-splitting. Could produce `pollution` on clean semantic chunks\n",
    "# - by_title specific params\n",
    "combine_text_under_n_chars: Optional[int] = None # Default to `max_characters`. Combines elements until a section reaches a length of n characters. Capped at `new_after_n_chars`\n",
    "multipage_sections: Optional[bool] = None # If True, sections can span multiple pages. Defaults to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider other chunking parameters\n",
    "parser_apply_chunking = CustomParseUnstructured(\n",
    "    mode=\"elements\",\n",
    "    post_processors=None, # UDF post-processors to be applied to resulting elements coming from the parser\n",
    "    # Following kwargs will be added to the unstructred_kwargs dict\n",
    "    chunking_strategy=\"by_title\",\n",
    "    include_orig_elements=include_orig_elements,\n",
    "    max_characters=max_characters,\n",
    "    new_after_n_chars=new_after_n_chars,\n",
    "    overlap=overlap,\n",
    "    overlap_all=overlap_all,\n",
    "    combine_text_under_n_chars=combine_text_under_n_chars,\n",
    "    multpage_sections=multipage_sections,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_chunks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_chunks = proposals.select(\n",
    "    refId=pw.this.refId,\n",
    "    content=parser_apply_chunking(\n",
    "        pw.apply_with_type(lambda x: f\"{x.as_str()}\".encode() if x else b\"\", bytes, pw.this.data.get(\"content\", default=None))\n",
    "    ),\n",
    ")\n",
    "proposal_chunks = proposal_chunks.flatten(pw.this.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_chunks.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking Analysis\n",
    "\n",
    "- Groupung chunks by proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_c = proposal_chunks.groupby(proposal_chunks.refId).reduce(proposal_chunks.refId, contents=json_acc(proposal_chunks.content))\n",
    "grouped_c.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.io.jsonlines.write(grouped_c, \"proposals-chunked.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatening contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using partition only\n",
    "# proposal_contents_splitted = proposal_contents.select(\n",
    "#     element_id=pw.this.content.get(\"element_id\", default=pw.Json(\"\")).as_str(),\n",
    "#     text=pw.this.content.get(\"text\", default=pw.Json(\"\")).as_str(),\n",
    "#     metadata=append_parent_id(pw.this.content[\"metadata\"], pw.this.refId),\n",
    "#     type=pw.this.content.get(\"type\", default=pw.Json(\"\")).as_str(),\n",
    "# )\n",
    "\n",
    "# Using partition + chunking\n",
    "proposal_contents_splitted = proposal_chunks.select(\n",
    "    element_id=pw.this.content.get(\"element_id\", default=pw.Json(\"\")).as_str(),\n",
    "    text=pw.this.content.get(\"text\", default=pw.Json(\"\")).as_str(),\n",
    "    metadata=append_parent_id(pw.this.content[\"metadata\"], pw.this.refId),\n",
    "    type=pw.this.content.get(\"type\", default=pw.Json(\"\")).as_str(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_contents_splitted.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_contents_splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_table = proposals_table.concat_reindex(proposal_contents_splitted)\n",
    "proposals_table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.io.jsonlines.write(proposals_table, \"proposals.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_API_KEY = \"empty-api-key\" # NOTICE: can't be empty otherwise python API throws an error\n",
    "EMBEDDINGS_MODEL = \"Nomic-embed-text-v1.5\"\n",
    "EMBEDDINGS_API_BASE = \"https://llama3.gaianet.network/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = CustomLiteLLMEmbedder(\n",
    "    api_base=EMBEDDINGS_API_BASE,\n",
    "    api_key=EMBEDDINGS_API_KEY,\n",
    "    custom_llm_provider=\"openai\", # litellm will use the .llms.openai.OpenAIChatCompletion to make the request\n",
    "    model=EMBEDDINGS_MODEL,\n",
    "    # NOTICE: tune parallelization\n",
    "    # capacity=5,\n",
    "    # retry_strategy=pw.asynchronous.udfs.FixedDelayRetryStrategy(delay_ms=10000),\n",
    "    # cache_strategy=pw.udfs.DefaultCache(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_vector = proposals_table.select(\n",
    "    text=pw.this.text,\n",
    "    embedding=embedder(pw.this.text),\n",
    "    metadata=pw.this.metadata,\n",
    ")\n",
    "proposals_vector.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes: data sources should match schema (data: bytes, _metadata: any)\n",
    "\n",
    "# doc_store = VectorStoreServer(\n",
    "#     # *data_sources(configuration[\"sources\"]),\n",
    "#     # *data_sources, # TODO:\n",
    "#     embedder=embedder,\n",
    "#     # splitter=splitters.TokenCountSplitter(max_tokens=400),\n",
    "#     parser=parser,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = KNNIndex(\n",
    "#     enriched_documents.vector, enriched_documents, n_dimensions=embedding_dimension\n",
    "# )\n",
    "# ...\n",
    "# query += query.select(\n",
    "#     vector=embedder(pw.this.query),\n",
    "# )\n",
    "\n",
    "# query_context = query + index.get_nearest_items(\n",
    "#     query.vector, k=3, collapse_rows=True\n",
    "# ).select(documents_list=pw.this.chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Voter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Voters(pw.Schema):\n",
    "#     data: pw.Json\n",
    "\n",
    "## TODO: fix this voter -> voters as data : voters[] @santiago\n",
    "    \n",
    "voters_raw = pw.io.http.read(\n",
    "    f\"https://api.boardroom.info/v1/protocols/{protocol_name}/voters?key={api_key}\",\n",
    "    method='GET',\n",
    "    headers={\"Accept\": \"application/json\"},\n",
    "    format=\"json\",\n",
    "    schema=BoardroomAPI,\n",
    "    # response_mapper=voter_mapper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voters_splitted = voters_raw.flatten(voters_raw.data)\n",
    "voters_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_all_protocols(protocols: pw.Json):\n",
    "    return \"\".join(protocol[\"protocol\"].as_str() + \", \" for protocol in protocols)\n",
    "\n",
    "def mapper(protocols: pw.Json):\n",
    "    for protocol in protocols:\n",
    "        if protocol[\"protocol\"].as_str() == protocol_name:\n",
    "            return protocol\n",
    "\n",
    "enhanced_voters = voters_splitted.select(\n",
    "    address=pw.this.data[\"address\"],\n",
    "    voter_protocol_data=pw.apply(mapper, pw.this.data[\"protocols\"]), \n",
    "    all_protocols=pw.apply(map_all_protocols, pw.this.data[\"protocols\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.debug.compute_and_print(enhanced_voters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delegations_raw = pw.io.http.read(\n",
    "    f\"https://api.boardroom.info/v1/delegates/getDelegatorsByProtocol/{protocol_name}?key={api_key}\",\n",
    "    method='GET',\n",
    "    headers={\"Accept\": \"application/json\"},\n",
    "    format=\"json\",\n",
    "    schema=BoardroomAPI\n",
    ")\n",
    "\n",
    "delegations_flattened = delegations_raw.flatten(delegations_raw.data)\n",
    "\n",
    "delegations = delegations_flattened.select(\n",
    "    adapter=pw.this.data[\"adapter\"],\n",
    "    delegatedFrom=pw.this.data[\"address\"],\n",
    "    delegatedTo=pw.this.data[\"addressDelegatedTo\"],\n",
    "    protocol=pw.this.data[\"protocol\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Delegate Pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "delegation_pitches_raw = pw.io.http.read(\n",
    "    f\"https://api.boardroom.info/v1/getDelegationPitchesByProtocol/{protocol_name}?key={api_key}\",\n",
    "    method=\"GET\",\n",
    "    headers={\"Accept\": \"application/json\"},\n",
    "    format=\"json\",\n",
    "    schema=BoardroomAPI\n",
    ")\n",
    "\n",
    "\n",
    "delegation_pitches_data = delegation_pitches_raw.select(\n",
    "    address = pw.this.data[\"delegationPitches\"]\n",
    ")\n",
    "\n",
    "delegation_pitches_flattened = delegation_pitches_data.flatten(delegation_pitches_data.address)\n",
    "\n",
    "\n",
    "delegation_pitches= delegation_pitches_flattened.select(\n",
    "    address= pw.this.address[\"address\"],\n",
    "    delegate_pitch = pw.this.address[\"delegationPitch\"],\n",
    "    protoocol = pw.this.address[\"protocol\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.debug.compute_and_print(delegation_pitches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture --no-display\n",
    "pw.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
