# GaiaNet RAG API Pipeline

`rag-api-pipeline` is a Python-based data pipeline tool that allows you to easily generate a vector knowledge base from any REST API data source. The
resulting database snapshot can then be plugged into a Gaia node's LLM model with a prompt and provide contextual responses to user queries using RAG 
(Retrieval Augmented Generation).

The following sections help you quickly set up and execute the pipeline on your REST API. If you're looking for more in-depth information about how to use 
this tool, the tech stack and/or how it works under the hood, check the content menu on the left.

## System Requirements

- Python 3.11.x
- Poetry ([Docs](https://python-poetry.org/docs/))
  - (Optional): a Python virtual environment manager of your preference (e.g. conda, venv)
- Qdrant vector database ([Docs](https://qdrant.tech/documentation/))
  - (Optional): Docker to spin up a local container
- LLM model provider (e.g. a [Public GaiaNet node](https://www.gaianet.ai/chat) or [spin up your own](https://docs.gaianet.ai/node-guide/quick-start))
  - An Embeddings model (e.g. [Nomic-embed-text-v1.5](https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/tree/main?show_file_info=nomic-embed-text-v1.5.f16.gguf))

## Setup Instructions

::::steps
### Clone this repository

Git clone or download this repository to your local machine.

```bash [Terminal]
git clone https://github.com/raid-guild/gaianet-rag-api-pipeline.git
```

### Install the Pipeline CLI 

It is recommended to activate your [own virtual environment](https://python-poetry.org/docs/basic-usage/#using-your-virtual-environment). 
Then, navigate to the directory where this repository was cloned/download and execute the following command to install the `rag-api-pipeline` CLI:

```bash [Terminal]
cd gaianet-rag-api-pipeline
pip install -e .
```

### Setup the Pipeline Configuration

Run the following command to start the pipeline setup wizard. You can use the default configuration settings or customize it for your specific needs.
Check the [CLI Reference](/cli-reference) page for more details:

```bash [Terminal]
rag-api-pipeline setup
```
::::

## Define your API Pipeline manifest

Now it's time to define the pipeline manifest for the REST API you're looking to extract data from. Check how to define an API pipeline manifest in 
[Defining an API Pipeline Manifest](/manifest-definition) for details, or take a look at the in-depth review of the sample manifests available in 
[API Examples](/apis).

## Pipeline CLI

Now you're ready to use the `rag-api-pipeline run` CLI commands to execute different tasks of the RAG pipeline, from extracting data from an API source to generating vector embeddings 
and a database snapshot. If you need more details about the parameters available on each command you can execute:

```bash [Terminal]
rag-api-pipeline run <command> --help
```

### CLI available commands

Below you can find the default instructions available and an in-depth review of both the functionality and available arguments that each command offers:

```bash [Terminal]
# run the entire pipeline
rag-api-pipeline run all <API_MANIFEST_FILE> <OPENAPI_SPEC_FILE> [--full-refresh]
# or run using an already normalized dataset
rag-api-pipeline run from-normalized <API_MANIFEST_FILE> --normalized-data-file <jsonl-file>
# or run using an already chunked dataset
rag-api-pipeline run from-chunked <API_MANIFEST_FILE> --chunked-data-file <jsonl-file>
```

- **run-all**: executes the entire RAG data pipeline including API endpoint data streams, data normalization, data chunking, vector embeddings and 
database snapshot generation. You can specify the following arguments to the command:
  * `API_MANIFEST_FILE`: API pipeline manifest file (mandatory)
  * `OPENAPI_SPEC_FILE`: API OpenAPI YAML spec file (mandatory)
  * `--llm-provider [ollama|openai]`: backend embeddings model provider. default: openai-like backend (e.g. gaia rag-api-server)
  * `--api-key`: API Auth key. If not specified, it will try to get it from `config/secrets/api_key`
  * `--source-manifest-file`: Airbyte API Connector YAML manifest. If specified, it will omit the API Connector manifest generation step.
  * `--full-refresh`: clean up cache and extract API data from scratch.
  * `--normalized-only`: run pipeline until the data normalization stage.
  * `--chunked-only`: run pipeline until the data chunking stage.

- **from-normalized**: executes the RAG data pipeline using an already normalized JSONL dataset. You can specify the following arguments to the command:
  * `API_MANIFEST_FILE`: API pipeline manifest file (mandatory)
  * `--llm-provider [ollama|openai]`: backend embeddings model provider. default: openai-like backend (e.g. gaia rag-api-server)
  * `--normalized-data-file`: path to the normalized dataset in JSONL format (mandatory). Check the [Architecture](/architecture) section for details on the 
  required data schema.

- **from_chunked**: executes the RAG data pipeline using an already chunked dataset in JSONL format. You can specify the following arguments to the command:
  * `API_MANIFEST_FILE`: API pipeline manifest file (mandatory)
  * `--llm-provider [ollama|openai]`: backend embeddings model provider. default: openai-like backend (e.g. gaia rag-api-server)
  * `--chunked-data-file`: path to the chunked dataset in JSONL format (mandatory). Check the [Architecture](/architecture) section for details on the 
  required data schema.

## CLI Output

Cached API stream data and results produced from running any of the CLI commands are stored in `<OUTPUT_FOLDER>/<api_name>`. The following files and folders 
are created by the tool within this `baseDir` folder:

- `{baseDir}/{api_name}_source_generated.yaml`: generated Airbyte Stream connector manifest.
- `{baseDir}/cache/{api_name}/*`: extracted API data is cached into a local DuckDB. Database files are stored in this directory. If the `--full-refresh` argument
is specified for the `run-all` command, the cache will be cleared and API data will be extracted from scratch.
- `{baseDir}/{api_name}_stream_{x}_preprocessed.jsonl`: data streams from each API endpoint are preprocessed and stored in JSONL format
- `{baseDir}/{api_name}_normalized.jsonl`: preprocessed data streams from each API endpoint are joined together and stored in JSONL format
- `{baseDir}/{api_name}_chunked.jsonl`: normalized data that goes through the data chunking stage is then stored in JSONL format
- `{baseDir}/{api_name}_collection-xxxxxxxxxxxxxxxx-yyyy-mm-dd-hh-mm-ss.snapshot`: vector embeddings snapshot file that was exported from Qdrant DB
- `{baseDir}/{api_name}_collection-xxxxxxxxxxxxxxxx-yyyy-mm-dd-hh-mm-ss.snapshot.tar.gz`: compressed knowledge base that contains the vector embeddings snapshot

## Environment variables

The following environment variables can be adjusted in `config/.env` based on user needs:

- Pipeline config parameters:
  - `API_DATA_ENCODING`: data encoding used by the REST API
    - Default value:  `utf-8`
  - `OUTPUT_FOLDER`: output folder where cached data streams, intermediary stage results and generated knowledge base snapshot are stored
    - Default value:  `./output`
- LLM provider settings:
  - `LLM_API_BASE_URL`: LLM provider base URL (defaults to a local openai-based provider such as gaia node)
    - Default value:  `http://localhost:8080/v1`
  - `LLM_API_KEY`: API key to authenticate requests to the LLM provider
    - Default value:  `empty-api-key`
  - `LLM_EMBEDDINGS_MODEL`: name of the embeddings model to be consumed through the LLM provider
    - Default value:  `Nomic-embed-text-v1.5`
  - `LLM_EMBEDDINGS_VECTOR_SIZE`: embeddings vector size
    - Default value:  `768`
  - `LLM_PROVIDER`: LLM provider backend to use. It can be either `openai` or `ollama` (gaianet offers an openai compatible API)
    - Default value:  `openai`
- Qdrant DB settings:
  - `QDRANTDB_URL`: Qdrant DB base URL
    - Default value:  `http://localhost:6333`
  - `QDRANTDB_TIMEOUT`: timeout for requests made to the Qdrant DB
    - Default value:  `60`
  - `QDRANTDB_DISTANCE_FN`: score function to use during vector similarity search. Available functions: ['COSINE', 'EUCLID', 'DOT', 'MANHATTAN']
    - Default value:  `COSINE`
- Pathway-related variables:
  - `AUTOCOMMIT_DURATION_MS`: the maximum time between two commits. Every autocommit_duration_ms milliseconds, the updates received by the connector are 
  committed automatically and pushed into Pathway's dataflow. More information can be found [here](https://pathway.com/developers/user-guide/connect/connectors/custom-python-connectors#connector-method-reference)
    - Default value:  `1000`
  - `FixedDelayRetryStrategy` ([docs](https://pathway.com/developers/api-docs/udfs#pathway.udfs.FixedDelayRetryStrategy)) config parameters:
    - `PATHWAY_RETRY_MAX_ATTEMPTS`: max retries to be performed if a UDF async execution fails
      - Default value:  `10`
    - `PATHWAY_RETRY_DELAY_MS`: delay in milliseconds to wait for the next execution attempt
      - Default value:  `1000`
  - *UDF async execution*: set the maximum number of concurrent operations per batch during UDF async execution. Zero means no specific limits. Be careful when setting
  these parameters for the embeddings stage as it could break the LLM provider with too many concurrent requests
    - `CHUNKING_BATCH_CAPACITY`: max number of concurrent operations during data chunking operations
      - Default value:  `0`
    - `EMBEDDINGS_BATCH_CAPACITY`: max number of concurrent operations during vector embeddings operations
      - Default value:  `15`


## Using Docker compose for Local development or in Production

TBD

- Start with building your containers: `docker compose -f local.yml build`.

- Build production containers with `docker compose -f prod.yml build`

- To run your application invoke:
    1. `docker compose -f prod.yml rm -svf`
    2. `docker compose -f prod.yml up`

## Troubleshooting

### Workaround in case of missing one of the following dependencies:

- If trying to install `pillow-heif` missing module:
  - Add the following flags `export CFLAGS="-Wno-nullability-completeness"`
- Libraries required for having libmagic working:
  - MacOS:
    - `brew install libmagic`
  - `pip install python-magic-bin`