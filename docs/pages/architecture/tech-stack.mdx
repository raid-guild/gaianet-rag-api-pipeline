# Tech Stack

This page outlines the technologies and tools integrated into the `rag-api-pipeline` across different execution stages.

## Tools & Frameworks

### 1. RAG Pipeline over Data Stream: Pathway ([Docs](https://pathway.com/developers/user-guide/introduction/welcome/))
- **Description**: A Python-based data processing framework designed for creating AI-driven pipelines over data streams 
- **Core Technology**:
  - **Rust Engine** with multithreading and multiprocessing capabilities for high performance
- **Use Case**: Efficient data processing, enabling integration with third-party data-related tools and AI models to process large, real-time data streams

### 2. Data Extraction from REST API Sources: PyAirbyte ([Docs](https://airbytehq.github.io/PyAirbyte/airbyte.html#getting-started)) + Airbyte CDK ([Docs](https://docs.airbyte.com/connector-development/config-based/low-code-cdk-overview))
- **Description**: Utilities for interacting with Airbyte declarative stream connectors using Python
- **Key Features**:
  - Facilitates integration with Airbyte data sources
  - Supports **Declarative API Connectors** via the Airbyte CDK, enabling low-code development of custom connectors

### 3. Data Caching: DuckDB ([Docs](https://airbytehq.github.io/PyAirbyte/airbyte/caches.html#DuckDBCache)) + Pathway JSONL Connector ([Docs](https://pathway.com/developers/user-guide/connect/connectors/jsonlines-connector))
- **Description**: Implements caching mechanisms at various pipeline stages to optimize performance and reduce redundant data processing
- **Technologies Used**:
  - **Airbyte DuckDB Cache**: Used for caching API data extractions, ensuring efficient retrieval of extracted data without re-querying the source
  - **JSONL Output Connectors**: After normalization and chunking, data is cached and stored in JSONL format, streamlining further processing stages

### 4. Data Partitioning and Chunking: Unstructured Open Source ([Docs](https://docs.unstructured.io/open-source/introduction/overview))
- **Description**: Simplifies the ingestion and preprocessing of diverse data formats within data workflows, specifically designed for **Large Language Models (LLMs)**
- **Features**:
  - Functions to **partition**, **chunk**, **clean**, and **stage** raw source documents for further analysis
  - Optimized for unstructured data handling, making it easier to prepare data for machine learning tasks

### 5. Feature Embedding Generation: Gaianet Node ([Docs](https://docs.gaianet.ai/category/node-operator-guide)) or Ollama ([Docs](https://ollama.com/))
- **Description**: An LLM provider responsible for generating feature embeddings, which create dense vector representations of the data. These embeddings are used downstream in vector search and other AI models
- **Technologies Used**:
  - **Gaianet Node**: Offers a *RAG API Server* that provides an *OpenAI-like API* to interact with hosted LLM models
  - **Ollama**: Easy-to-install LLM engine for running large language models on a local machine
- **Python Libraries**:
  - [litellm](https://docs.litellm.ai/docs/providers/openai_compatible) Python library for connecting with OpenAI-compatible LLM providers
  - [ollama](https://github.com/ollama/ollama-python) Python library for interacting with a local Ollama instance

### 6. Vector Embeddings Database Snapshot: QdrantDB ([Docs](https://qdrant.tech/documentation/))
- **Description**: A **vector database** and **vector similarity search engine**
- **Key Features**:
  - Provides efficient vector searches based on similarity, crucial for tasks like nearest-neighbor search in large datasets
  - Acts as a **knowledge base snapshot** repository, storing vectors generated from processed data and feature embeddings