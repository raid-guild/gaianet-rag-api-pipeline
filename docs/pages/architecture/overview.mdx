# Architecture Overview

![architecture](https://raw.githubusercontent.com/raid-guild/gaianet-rag-api-pipeline/refs/heads/main/docs/public/architecture.png)

The diagram above illustrates the system architecture. When executing the `rag-api-pipeline` CLI, the tool performs the following steps:

1. The API `Loader` module reads both the API pipeline manifest and the OpenAPI specification.
2. The `Loader` module generates an Airbyte declarative stream manifest and forwards it to the `Pipeline` module.
3. The `Pipeline` module initiates the Pathway engine.
4. The engine uses an `Airbyte UDF connector` module to extract data from each API endpoint as independent streams.
5. Extracted data flows through the RAG pipeline stages: stream preprocessing, data normalization, data partitioning and chunking, and feature embeddings generation.
    - (5.4) The `Feature Embeddings` generation module can be configured to use an OpenAI-compatible API provider, such as a Gaia node, to generate vector embeddings with your preferred LLM embeddings model. Visit the [Other LLM Providers](/cli/other-llm-providers) page for details on other supported LLM providers.
    - (5.5) Generated vector embeddings are stored in a Qdrant DB instance using the `qdrant` output module.
6. The `qdrant` module automatically generates and compresses a DB vector snapshot for the current vector embeddings collection.
7. The resulting knowledge base snapshot can be imported into a Gaia node either through local upload or by uploading to an AI dataset/model provider such as HuggingFace.
8. The node joins the Gaia network to provide an LLM model with custom domain knowledge that can be used to deliver various RAG applications to end users.