# In-depth Source Code Review

This document walks through the different modules that comprise the RAG (Retrieval-Augmented Generation) API Pipeline architecture. The system is designed to extract, preprocess, parse, and store data streams from an API source, then create a vector embeddings knowledge base that can be queried using natural language processing and/or hybrid search techniques. Data from each pipeline stage is serialized using a JSON Lines format for easy caching and reuse.

## Data Flow Overview

<iframe src="https://mermaid.live/view#pako:eNplVF1vozAQ_CuWXy8l4SPQoNNJiZqmVZM2Ou7pnD44eAOogJEx7eWa_PdbQ0lzqSUke3Z2Zz22eaexFEBDmihepeTXbFMSHHWz7YBpprZ7DWxDP2bkG1lznb7x_YY-d2Qz1iu2zirIsxLIipfZDmr9TK6ufhxsyyYKuDiQ6ZJN1_dLyQWos9SniD1VUGKIRBXEfZZzyvqkTpdt0MGSCZSguIYDiVYsko2KgRS98GdGtGozXMyoNVf6QH5OFww_0rd7RjawYXstWyos_jBjD6V8y0EkQGa8Pqc_zD7YTs_GAl0YSnFhJIYiUK-g0Eqj0263Bf7zcblcMfxIgaeSn5s0ve9cujeZZ4H5is2LLQiRlcmXrItmZgs2k1wJJWVBFhK1S16ib6ai2Up_2h27d2NsjcgOuG4UEOilDp8b6tgfZvhIjkpe1anUXzgXy8UjW_CMP4ImayW1jGVOcPEm1cvzRQL5btpbPHbw3S27a5LE7PiWx4C9WyhtLtYXyZP_S0hOPrRG2wxFY6jrc_cddsM1J5E5znPcZfM_2tiVt4eWxX0Ua9IBLUAVPBP4jN4NvKE6hQI2NMSp4OplQzflEXm80TLalzENtWpgQJVskpSGO57XuGoqgRf6JuPYcXGBzkWGPZ3AvH1ENHynel-1zzerNQrEstxlicEblSOcal3V4XBowlaS6bTZWrEshnUmUnwN6evEH_qOf80dF_zA5WPXFfHWnlzvHM_eiWBkO5wejwMKrf6q-1e0v4wBrXj5G2_SqSlcG-U_NPS8seU7I8_3XMcbjQN_PKB7GrquhdBoPPa8IJg49gQL_20rjKxrN_B8Z-Lb2Ebg-sHxHxjOZcQ"
width="100%"
height="900px">
</iframe>

1. **API Loader**: Generates an Airbyte declarative stream manifest using the input API pipeline manifest and the API OpenAPI spec.
2. **Data Extraction**: PyAirbyte uses the source manifest to create individual data streams for each specified API endpoint. Raw data is then cached using Airbyte DuckDB for efficient API data retrieval.
3. **Processing & Transformation**: Pathway handles real-time preprocessing of data streams, transforming raw data into a usable format. Endpoint text fields specified in the API pipeline manifest are joined together into a `content` field, while the remaining fields are stored in a JSON `metadata` object.
4. **Data Normalization**: Preprocessed data streams are joined together into a single normalized stream.
5. **Data Partitioning & Chunking**: Normalized data records are then partitioned and chunked.
6. **Vector Embeddings Generation**: The pipeline connects to an LLM provider to use a hosted embedding model for generating vector embeddings from chunked data records.
7. **Loading Knowledge Base Snapshot**: The resulting QdrantDB collection snapshot can be imported into a Gaia node, allowing the LLM model to use the domain knowledge for providing RAG-based answers to end users.

## Pipeline Components

### CLI Entrypoint ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/run.py))

The entrypoint module uses [Click](https://click.palletsprojects.com/en/8.1.x/#documentation) to define CLI commands that execute specific pipeline stages. Check the [CLI reference](/cli-reference) for details on available commands and arguments.

### API Loader ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/loader.py))

Uses the input API pipeline manifest and API OpenAPI spec to generate/load an Airbyte declarative stream manifest for initiating the input data streams.

Input Parameters:
- **API Pipeline Manifest**: A YAML file defining configuration settings and API endpoints for extraction. The [Defining the API Pipeline Manifest](/manifest-definition) reference provides details on creating a new manifest for your target API.
- **OpenAPI Spec**: YAML file containing the OpenAPI specification for the API source.

Output:
- **api_name**: API Pipeline ID
- **api_parameters**: Input parameter values for injection into the Airbyte API connector
- **source_manifest**: Generated source manifest (dict serialized)
- **endpoint_text_fields**: Text fields per API endpoint
- **chunking_params**: Chunking parameters for the data chunking stage

### Input Module ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/input.py))

Uses Pathway and PyAirbyte to implement the `AirbyteAPIConnector` ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/io/airbyte/api_connector.py)). The input module uses this custom connector with `api_parameters` to create data stream tables for each API endpoint in `source_manifest`.

### RAG Pipeline ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/pipeline.py))

Receives input data streams and executes the following processing steps:

1. **Preprocessing** ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/preprocessing.py)): Transforms raw data streams into a [unified data schema](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/schema/pipeline.py#L6) using specified `endpoint_text_fields`.

2. **Normalization**: Joins preprocessed data into a single normalized stream table.

3. **Data Partitioning and Chunking** ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/chunking.py)): Receives the normalized data stream and `chunking_params` to apply data partition and chunking using the `CustomParseUnstructured` ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/processor/parser.py)) UDF (Pathway User-defined Function).

4. **Feature Embeddings** ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/embeddings.py)): The `CustomLiteLLMEmbedder` ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/processor/embedder.py)) integrates `litellm` and `ollama` libraries to connect to the selected LLM provider for generating vector embeddings.

:::info
NOTICE:
For pipeline execution on consumer hardware, we recommend using Ollama as a more lightweight LLM provider for vector embeddings generation.
:::

### Output Module ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/output.py))

Uses the `qdrant_client` library to implement the `QdrantDBVectorStore` ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/io/qdrant.py)). It connects to a [Pathway output connector](https://pathway.com/developers/user-guide/connect/connectors/python-output-connectors) to read records from the embeddings stream, store vector embeddings and attached metadata in a Qdrant DB collection, and generate and download a knowledge base snapshot.