# In-depth Source Code Review

In the following sections, we'll walk you through the different modules that conform the RAG (Retrieval-Augmented Generation) API Pipeline architecture. 
The system is designed to extract, preprocess, parse and store data streams from an API source, then creates a vector embeddings knowledge base that can be 
queried using natural language processing and/or hybrid search techniques. Resulting data from each stage of the pipeline is serialized using a Json lines format, 
so they can be easily cached and reused.

## Data Flow Overview

<iframe src="https://mermaid.live/view#pako:eNplVF1vozAQ_CuWXy8l4SPQoNNJiZqmVZM2Ou7pnD44eAOogJEx7eWa_PdbQ0lzqSUke3Z2Zz22eaexFEBDmihepeTXbFMSHHWz7YBpprZ7DWxDP2bkG1lznb7x_YY-d2Qz1iu2zirIsxLIipfZDmr9TK6ufhxsyyYKuDiQ6ZJN1_dLyQWos9SniD1VUGKIRBXEfZZzyvqkTpdt0MGSCZSguIYDiVYsko2KgRS98GdGtGozXMyoNVf6QH5OFww_0rd7RjawYXstWyos_jBjD6V8y0EkQGa8Pqc_zD7YTs_GAl0YSnFhJIYiUK-g0Eqj0263Bf7zcblcMfxIgaeSn5s0ve9cujeZZ4H5is2LLQiRlcmXrItmZgs2k1wJJWVBFhK1S16ib6ai2Up_2h27d2NsjcgOuG4UEOilDp8b6tgfZvhIjkpe1anUXzgXy8UjW_CMP4ImayW1jGVOcPEm1cvzRQL5btpbPHbw3S27a5LE7PiWx4C9WyhtLtYXyZP_S0hOPrRG2wxFY6jrc_cddsM1J5E5znPcZfM_2tiVt4eWxX0Ua9IBLUAVPBP4jN4NvKE6hQI2NMSp4OplQzflEXm80TLalzENtWpgQJVskpSGO57XuGoqgRf6JuPYcXGBzkWGPZ3AvH1ENHynel-1zzerNQrEstxlicEblSOcal3V4XBowlaS6bTZWrEshnUmUnwN6evEH_qOf80dF_zA5WPXFfHWnlzvHM_eiWBkO5wejwMKrf6q-1e0v4wBrXj5G2_SqSlcG-U_NPS8seU7I8_3XMcbjQN_PKB7GrquhdBoPPa8IJg49gQL_20rjKxrN_B8Z-Lb2Ebg-sHxHxjOZcQ"
width="100%"
height="900px" >
</iframe>

1. **API Loader**: generates an Airbyte declarative stream manifest using the input API pipeline manifest and the API OpenAPI spec.
2. **Data Extraction**: PyAirbyte uses the source manifest to create individual data stream for each of the specified API endpoints. 
Raw data is then cached using Airbyte DuckDB for efficient API data retrieval.
2. **Processing & Transformation**: Pathway handles real-time pre-processing of data streams, transforming raw data into usable format. Endpoint text fields specified 
in the API pipeline manifest are joined together into a `content` field, while the rest of fields go into a JSON `metadata` object.
3. **Data Normalization**: pre-processed data streams are joined together into a single normalized stream.
4. â€¢*Data Partitioning & Chunking**: normalized data records are then partitioned and chunked.
5. **Vector Embeddings Generation**: the pipeline connects to an LLM provider to use a hosted embedding model to generate vector embeddings from chunked data records.
6. **Loading Knowledge Base Snapshot**: a resulting QdrantDB collection snapshot can then be imported into a Gaia node so the LLM model can use the domain knowledge 
to provide RAG-based answers to end users.

## Pipeline Components

### CLI entrypoint ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/run.py))

The entrypoint module uses [Click](https://click.palletsprojects.com/en/8.1.x/#documentation) to define different CLI commands that can be used to 
execute specific pipeline stages. Check the [CLI reference](/cli-reference) for more details on commands and arguments avaiable.

### API Loader ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/loader.py))

It uses the input API pipeline manifest and API OpenAPI spec to generate/load an Airbyte declarative stream manifest that be later used to kickstart the input data streams

Input Parameters:

- *API Pipeline Manifest*: A YAML file that defines the configuration settings and API endpoints for extraction. The [Defining the API Pipeline Manifest](/manifest-definition) 
reference provides details on how to create a new manifest for your target API.
- *OpenAPI Spec*: YAML file containing the OpenAPI specification for the API source.

Output:

- *api_name*: API Pipeline id
- *api_parameters*: input parameter values to be injected into the Airbyte API connector.
- *source_manifest*: generated source manifest (dict serialized)
- *endpoint_text_fields*: text fields per API endpoint
- *chunking_params*: chunking params that will be used during the data chunking stage.

### Input module ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/input.py))

It uses Pathway and PyAirbyte to implement the `AirbyteAPIConnector` ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/io/airbyte/api_connector.py)).
The input module then uses this custom connector with `api_parameters` for creating a data stream table for each API endpoint in `source_manifest`.

### RAG Pipeline ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/pipeline.py))

It receives a list of input data streams and execute the following processing steps:

1. **Preprocessing** ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/preprocessing.py)): transforms the 
raw data streams into a [unified data schema](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/schema/pipeline.py#L6) using 
the specified `endpoint_text_fields`.
2. **Normalization**: in charge of joining pre-processed data together into a single normalized stream table.
3. **Data partitioning and chunking** ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/chunking.py)): receives 
the normalized data stream and `chunking params` to apply data partition and chunking to each record in the stream using the 
`CustomParseUnstructured` ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/processor/parser.py)) UDF (Pathwar User-defined function). 
This function uses the `unstructured` open-source library to semantically parse, partition and chunk incoming data records.
4. **Feature embeddings** ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/embeddings.py)): the 
`CustomLiteLLMEmbedder` ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/processor/embedder.py)) integrates 
`litellm` and `ollama` libraries to connect to the selected LLM provider and use the hosted LLM embeddings model to generate vector embeddings for every 
record coming from the chunked data stream.

:::info
NOTICE:
If you're planning to execute the pipeline on consumer hardware, we recommend using an Ollama as a more-lighweight LLM provider for vector embeddings generation.
:::

### Output module ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/output.py))

It uses the `qdrant_client` library to implement the `QdrantDBVectorStore` ([Source](https://github.com/raid-guild/gaianet-rag-api-pipeline/blob/main/gaianet_rag_api_pipeline/io/qdrant.py)). 
It is then wired to a [Pathway output connector](https://pathway.com/developers/user-guide/connect/connectors/python-output-connectors) to read records from the
embeddings stream, store vector embeddings and attached metadata into a collection in Qdrant DB, and finally generate and download a knowledge base snapshot.
