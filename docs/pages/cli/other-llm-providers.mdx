# Supported LLM providers

The `rag-api-pipeline` currently supports two types of LLM providers: `openai` and `ollama`. A Gaianet node for example, uses a Rust-based [RAG API Server](https://github.com/LlamaEdge/rag-api-server) 
to offer OpenAI-compatible web APIs for creating RAG applications.

In the following sections, you'll find more details on the supported LLM providers that the pipeline currently supports and how to setup them

## OpenAI

By default, the pipeline supports any LLM provider that offers OpenAI-compatible web APIs. If you wanna work with a provider other than Gaianet, 
you can setup the connection using the setup wizard via the `rag-api-pipeline setup` command:

```bash [Terminal]
(venv) user$ rag-api-pipeline setup
Which LLM provider you want to use? (gaia, other) [gaia]: other
Init pipeline...
(Step 1/3) Setting Pipeline LLM provider settings...
Select a custom LLM provider (openai, ollama): openai
LLM provider API URL [http://127.0.0.1:8080/v1]: https://api.openai.com/v1
LLM provider API Key: 
LLM Provider API connection OK!
Embeddings model Name [Nomic-embed-text-v1.5]: text-embedding-ada-002
Embeddings Vector Size [768]: 2048
Pipeline LLM Provider settings OK!
```

## Ollama

If you're planning to use the pipeline on consumer hardware that cannot handle a GaiaNet node running in the background, you can opt-in to use Ollama 
as LLM provider. Depending on the use case and resources available, some of the advantages of using Ollama for example are that it is more lighweight, 
easier to install and ready to use with Mac GPU devices.

### Getting Ollama

Download and install Ollama from the official [website](https://ollama.com/download).

### Importing an Embeddings Model in Ollama

### Using the CLI Setup Wizard

Using the `rag-api-pipeline setup` command, you can easily set the connection to Ollama and even load the embeddings model if it's not imported already.

```bash [Terminal]
(venv) user$ rag-api-pipeline setup
Which LLM provider you want to use? (gaia, other) [gaia]: other
Init pipeline...
(Step 1/3) Setting Pipeline LLM provider settings...
Select a custom LLM provider (openai, ollama): ollama
LLM provider API URL [http://127.0.0.1:11434]: 
ERROR: LLM Provider API (@ http://127.0.0.1:11434/v1/models) is down. HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1091c2490>: Failed to establish a new connection: [Errno 61] Connection refused'))
Try again...
LLM provider API URL [http://127.0.0.1:11434]:
LLM Provider API connection OK!
Embeddings model Name [Nomic-embed-text-v1.5]: 
Embeddings Vector Size [768]: 
Enter the Absolute Path to the Embeddings model file: /home/user/rag-api-pipeline/models/nomic-embed-text-v1.5.f16.gguf
Importing embeddings model into Ollama...
Pipeline LLM Provider settings OK!
```

### Manually Importing a Model

1. Download the embeddings model of your preference (e.g. from [HuggingFace](https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf))
2. Create `Modelfile` to use the embedding model with Ollama. Learn more about Ollama Modelfile format [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)

```docker [Modelfile]
FROM ./nomic-embed-text-v1.5.f16.gguf # this is the path to the embedding model
```

3. Import the model into Ollama using the `ollama` CLI

```bash [Terminal]
ollama create <model-name> -f <Modelfile>
```
