# RAG API Pipeline CLI Reference Documentation
## Overview
The CLI tool provides functionality for running a RAG (Retrieval-Augmented Generation) API pipeline. It offers various commands to execute different stages of the pipeline, from data extraction to embedding generation.

## Installation
This project uses Poetry for dependency management. To install the project and all its dependencies:
1. Ensure you have Poetry installed. If not, install it by following the instructions [here](https://python-poetry.org/docs/#installation)
2. Clone the repository:
```bash [Terminal]
git clone https://github.com/raid-guild/gaianet-rag-api-pipeline
cd gaianet-rag-api-pipeline
```
3. Install dependencies using Poetry:
```bash [Terminal]
poetry install
```
This will create a virtual environment and install all necessary dependencies specified in the `pyproject.toml` file.

## Usage
To run any command, use the `poetry run` prefix:
```bash [Terminal]
poetry run rag-api-pipeline [OPTIONS] COMMAND [ARGS]...
```

## Global Options
- `--debug`: Enable logging at debug level. Useful for development purposes

## Commands
### run-all
Run the complete RAG API pipeline.
```bash [Terminal]
poetry run rag-api-pipeline run-all [OPTIONS] API_MANIFEST_FILE
```
#### Arguments
- `API_MANIFEST_FILE`: Pipeline YAML manifest that defines the Pipeline config settings and API endpoints to extract.
#### Options
- `--llm-provider [ollama|openai]`: Embedding model provider (default: openai)
- `--api-key TEXT`: API Auth key
- `--openapi-spec-file FILE`: OpenAPI YAML spec file (default: config/openapi.yaml)
- `--source-manifest-file FILE`: Source Connector YAML manifest
- `--full-refresh`: Clean up cache and extract API data from scratch
- `--normalized-only`: Run pipeline until the normalized data stage
- `--chunked-only`: Run pipeline until the chunked data stage

### from-normalized
Executes the RAG data pipeline using an already normalized JSONL dataset.
```bash [Terminal]
poetry run rag-api-pipeline from-normalized [OPTIONS] API_MANIFEST_FILE
```
#### Arguments
- `API_MANIFEST_FILE`: Pipeline YAML manifest that defines the Pipeline config settings and API endpoints to extract.
#### Options
- `--llm-provider [ollama|openai]`: Embedding model provider (default: openai)
- `--normalized-data-file FILE`: Normalized data in JSONL format (required)

### from-chunked
Executes the RAG data pipeline using an already chunked dataset in JSONL format.
```bash [Terminal]
poetry run rag-api-pipeline from-chunked [OPTIONS] API_MANIFEST_FILE
```
#### Arguments
- `API_MANIFEST_FILE`: Pipeline YAML manifest that defines the Pipeline config settings and API endpoints to extract.
#### Options
- `--llm-provider [ollama|openai]`: Embedding model provider (default: openai)
- `--chunked-data-file FILE`: Chunked data in JSONL format (required)

## Examples
1. Run the complete pipeline using Ollama as LLM provider:
```bash [Terminal]
poetry run rag-api-pipeline run-all config/api_pipeline.yaml --openapi-spec-file config/openapi.yaml --llm-provider ollama
```
2. Run the pipeline and stop executing after data normalization:
```bash [Terminal]
poetry run rag-api-pipeline run-all config/api_pipeline.yaml --openapi-spec-file config/openapi.yaml --llm-provider ollama --normalized-only
```
3. Run the pipeline from normalized data:
```bash [Terminal]
poetry run rag-api-pipeline from-normalized config/api_pipeline.yaml --normalized-data-file path/to/normalized_data.jsonl --llm-provider ollama
```
4. Run the pipeline from chunked data:
```bash [Terminal]
poetry run rag-api-pipeline from-chunked config/api_pipeline.yaml --chunked-data-file path/to/chunked_data.jsonl --llm-provider ollama
```

## Notes
- Always use `poetry run` to execute the CLI commands within the Poetry environment.
- The CLI uses the `click` library for command-line interface creation.
- Make sure to properly configure your API manifest file and OpenAPI spec file before running the pipeline.
- Double-check the CLI environment variables in the `config/.env` file. Check the [Environment Variables](/getting-started#environment-variables) section for details.


## Moved from getting started

TODO: 

- **run-all**: executes the entire RAG data pipeline including API endpoint data streams, data normalization, data chunking, vector embeddings and 
database snapshot generation. You can specify the following arguments to the command:
  * `API_MANIFEST_FILE`: API pipeline manifest file (mandatory)
  * `OPENAPI_SPEC_FILE`: API OpenAPI YAML spec file (mandatory)
  * `--llm-provider [ollama|openai]`: backend embeddings model provider. default: openai-like backend (e.g. gaia rag-api-server)
  * `--api-key`: API Auth key. If not specified, it will try to get it from `config/secrets/api_key`
  * `--source-manifest-file`: Airbyte API Connector YAML manifest. If specified, it will omit the API Connector manifest generation step.
  * `--full-refresh`: clean up cache and extract API data from scratch.
  * `--normalized-only`: run pipeline until the data normalization stage.
  * `--chunked-only`: run pipeline until the data chunking stage.

- **from-normalized**: executes the RAG data pipeline using an already normalized JSONL dataset. You can specify the following arguments to the command:
  * `API_MANIFEST_FILE`: API pipeline manifest file (mandatory)
  * `--llm-provider [ollama|openai]`: backend embeddings model provider. default: openai-like backend (e.g. gaia rag-api-server)
  * `--normalized-data-file`: path to the normalized dataset in JSONL format (mandatory). Check the [Architecture](/architecture) section for details on the 
  required data schema.

- **from_chunked**: executes the RAG data pipeline using an already chunked dataset in JSONL format. You can specify the following arguments to the command:
  * `API_MANIFEST_FILE`: API pipeline manifest file (mandatory)
  * `--llm-provider [ollama|openai]`: backend embeddings model provider. default: openai-like backend (e.g. gaia rag-api-server)
  * `--chunked-data-file`: path to the chunked dataset in JSONL format (mandatory). Check the [Architecture](/architecture) section for details on the 
  required data schema.

## CLI Output

Cached API stream data and results produced from running any of the CLI commands are stored in `<OUTPUT_FOLDER>/<api_name>`. The following files and folders 
are created by the tool within this `baseDir` folder:

- `{baseDir}/{api_name}_source_generated.yaml`: generated Airbyte Stream connector manifest.
- `{baseDir}/cache/{api_name}/*`: extracted API data is cached into a local DuckDB. Database files are stored in this directory. If the `--full-refresh` argument
is specified for the `run-all` command, the cache will be cleared and API data will be extracted from scratch.
- `{baseDir}/{api_name}_stream_{x}_preprocessed.jsonl`: data streams from each API endpoint are preprocessed and stored in JSONL format
- `{baseDir}/{api_name}_normalized.jsonl`: preprocessed data streams from each API endpoint are joined together and stored in JSONL format
- `{baseDir}/{api_name}_chunked.jsonl`: normalized data that goes through the data chunking stage is then stored in JSONL format
- `{baseDir}/{api_name}_collection-xxxxxxxxxxxxxxxx-yyyy-mm-dd-hh-mm-ss.snapshot`: vector embeddings snapshot file that was exported from Qdrant DB
- `{baseDir}/{api_name}_collection-xxxxxxxxxxxxxxxx-yyyy-mm-dd-hh-mm-ss.snapshot.tar.gz`: compressed knowledge base that contains the vector embeddings snapshot
