# RAG API Pipeline x GaiaNet node

This page contains some quick notes and instructions for efficient methods for generating vector embeddings, as well as quick instructions on how to import a knowledge base snapshot and deploy a GaiaNet node.

## Generating Vector embeddings

If you're planning to use the pipeline on consumer hardware that cannot handle a GaiaNet node running in the background, you can opt-in to use Ollama 
as LLM provider. Some of the advantages are that it is more lighweight, easier to install and ready to use with Mac GPU devices.

### Setting up Ollama

Download and install ollama from the official [website](https://ollama.com/download)

### Setting up Embeddings Model with Ollama

1. Download the embeddings model of your preference (e.g. from [HuggingFace](https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf))
2. Create `Modelfile` to use the embedding model with Ollama. Learn more about Ollama Modelfile [here](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)

```docker [Modelfile]
FROM ./nomic-embed-text-v1.5.f16.gguf # this is the path to the embedding model
```

3. Import the model into Ollama

```bash [Terminal]
ollama create <model-name> -f <Modelfile>'
```

## Selecting a knowledge base and prompts for your GaiaNet node

In order to supplement the LLM model hosted on your Gaia node with a custom knowledge base and prompts follow the instructions outlined in this [link](https://docs.gaianet.ai/node-guide/customize#select-a-knowledge-base). 
Remember to re-initialize and re-start the node after you make configuration changes.

```bash [Terminal]
gaianet init
gaianet start
```

### Recommended GaiaNet Node Configuration

- Tested on Mac Studio 32GB RAM
- Custom prompts for the [Boardroom Aave example](/apis/boardroom-api)

```json [config.json]
{
  "address": "your-node-address",
  "chat": "https://huggingface.co/gaianet/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf",
  "chat_batch_size": "64",
  "chat_ctx_size": "8192",
  "chat_name": "Boardroom-Llama-3-Chat",
  "description": "Llama-3-chat model. with Boardroom API snapshot",
  "domain": "us.gaianet.network",
  "embedding": "https://huggingface.co/gaianet/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5.f16.gguf",
  "embedding_batch_size": "2048",
  "embedding_collection_name": "boardroom_api_collection", # this is the name of the collection in the snapshot
  "embedding_ctx_size": "2048",
  "embedding_name": "Nomic-embed-text-v1.5",
  "llamaedge_port": "8080",
  "prompt_template": "llama-3-chat",
  "qdrant_limit": "1",
  "qdrant_score_threshold": "0.5",
  "rag_policy": "system-message",
  "rag_prompt": "Use the following pieces of context to answer the user's question. Respond directly to the user with your answer, do not say 'this is the answer' or 'this is the answer' or similar language. Never mention your knowledge base or say 'according to the context' or 'hypothetical' or other similar language. Use json metadata included in knowledge base whenever possible enrich your answers. The term aave refers the DAO protocol where discussions and proposals are posted. If you don't know the answer, don't try to make up an answer. \n----------------\n",
  "reverse_prompt": "",
  "snapshot": "/your-snapshot-path-or-url",
  "system_prompt": "You are an AI assistant designed to provide clear, concise, and accurate answers to user queries. Your primary functions include retrieving relevant information from the provided RAG (Retrieval-Augmented Generation) data and utilizing your pre-training data when necessary. Use json metadata included in RAG data whenever possible enrich your answers. The term aave refers the DAO protocol where discussions and proposals are posted. If no relevant information is found, you will inform the user that you are not familiar with the knowledge."
}
```
